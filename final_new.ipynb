{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"markdown","source":["# ***Problem statement***"],"metadata":{"id":"SRgPx6fclI9B"}},{"cell_type":"markdown","source":["In this paper, we explore the existing research on Transformer,Bert,ROBERTA,XLM,ALBERT. and proposing potentially more efficient and scalable architectures for processing and generating natural language text that can achieve state-of-the-art performance on a variety of natural language processing (NLP) tasks.\n","\n","This problem is important because natural language understanding is a fundamental task of artificial intelligence with a wide range of applications in areas such as machine translation, sentiment analysis, and chatbots. It is also difficult because natural language is highly complex, with many linguistic nuances that require a deep understanding of context and semantics.\n","\n","The main problem addressed by the Transformer-based models is language understanding, which involves capturing the contextual relationship between words and their representations in large-scale corpora. Traditional models such as recurrent neural networks and convolutional neural networks were not efficient in capturing these long-range dependencies. Hence, the Transformer architecture was introduced, which relies on self-attention mechanisms to attend to different parts of the input sequence to create a better representation.\n","\n","\n","本文通过探讨Transformer的延伸,Bert,ROBERTA,XLM,ALBERT的已有研究.并提出潜在更有效和可扩展的架构来处理和生成自然语言文本的方法，能够在各种自然语言处理（NLP）任务上达到最先进的性能。\n","\n","这个问题很重要，因为自然语言理解是人工智能的一项基本任务，在机器翻译、情感分析和聊天机器人等领域有广泛的应用。它也很难，因为自然语言是高度复杂的，有许多语言上的细微差别，需要对语境和语义有深刻的理解。\n","基于Transformer的模型所解决的主要问题是语言理解，这涉及到在大规模语料库中捕获单词和它们的表示之间的上下文关系。传统的模型，如递归神经网络和卷积神经网络，在捕捉这些长距离的依赖关系方面并不高效。因此，引入了Transformer架构，它依靠自我注意机制来关注输入序列的不同部分，以创建一个更好的表示。"],"metadata":{"id":"rrbQ-I7flHCE"}},{"cell_type":"markdown","source":["# ***Main result***"],"metadata":{"id":"mPN3ONnRlNlt"}},{"cell_type":"markdown","source":["The main contribution of Transformer-based models is the pre-training technique, where the model is trained on a large corpus of text to learn the language's representation. BERT, RoBERTa, XLM, and ALBERT are examples of pre-trained models that have shown state-of-the-art performance in various NLP tasks.\n","\n","The pre-training technique involves two stages: (i) pre-training a transformer-based model on a large corpus of text, and (ii) fine-tuning the pre-trained model on specific NLP tasks. The pre-training stage is unsupervised, where the model learns the language's representation by predicting the masked words and next sentence prediction tasks.\n","\n","The fine-tuning stage involves training the pre-trained model on specific NLP tasks, such as sentiment analysis or question answering. The pre-trained model's weights are fine-tuned on the specific task's dataset, and the model is then used to predict the task's output.\n","主要结果：\n","基于Transformer的模型的主要贡献是预训练技术，即在大量的文本语料库中训练模型以学习语言的表示。BERT、RoBERTa、XLM和ALBERT是预训练模型的例子，它们在各种NLP任务中表现出最先进的性能。\n","\n","预训练技术包括两个阶段：(i) 在大型文本语料库上预训练基于转化器的模型，以及(ii) 在特定的NLP任务上微调预训练模型。预训练阶段是无监督的，模型通过预测被掩盖的词和下一个句子的预测任务来学习语言的表示。\n","\n","微调阶段包括在特定的NLP任务上训练预训练的模型，如情感分析或问题回答。预训练模型的权重在特定任务的数据集上进行微调，然后用该模型来预测任务的输出。"],"metadata":{"id":"cXhXC0mulOo9"}},{"cell_type":"markdown","source":["# ***Examples and counterexamples***"],"metadata":{"id":"pDHT-I6klP9z"}},{"cell_type":"markdown","source":["Example: In a machine translation task, the Transformer architecture is able to capture complex relationships between words in the source and target languages, resulting in more accurate translations.\n","\n","Counterexample: If the input text has significant noise or is from a domain that the model has not been trained on, the performance of the Transformer model may be negatively impacted. For instance, if the model has been trained on news articles but is applied to informal social media text, it may struggle to generate coherent and contextually accurate responses.\n","\n","例子： 在机器翻译任务中，Transformer架构能够捕捉到源语言和目标语言中的复杂关系，从而使翻译更加准确。\n","\n","反例： 如果输入的文本有明显的噪音，或者来自于模型没有被训练过的领域，Transformer模型的性能可能会受到负面的影响。例如，如果该模型已经在新闻文章上进行了训练，但被应用于非正式的社交媒体文本，它可能难以产生连贯和上下文准确的反应。"],"metadata":{"id":"fE5Icn3LlSHI"}},{"cell_type":"markdown","source":["# ***Empirical Studies***"],"metadata":{"id":"jW0GbZeplW7W"}},{"cell_type":"markdown","source":["经验研究表明，在各种NLP任务中，预训练的基于Transformer的模型，如BERT、RoBERTa、XLM和ALBERT的表现超过了传统模型，包括情感分析、机器翻译和问题回答。这些模型在GLUE、SQuAD和CoNLL等基准数据集上取得了最先进的性能。"],"metadata":{"id":"7IMYAnQtlZie"}},{"cell_type":"markdown","source":["# 进行复刻"],"metadata":{"id":"paKZqeL9HyOV"}},{"cell_type":"code","source":["\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JbbKuC72bU04","executionInfo":{"status":"ok","timestamp":1680228899054,"user_tz":240,"elapsed":22235,"user":{"displayName":"kuma","userId":"09741713883164986443"}},"outputId":"9bf9b1bf-8ab1-4851-c4e0-13e13111913d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["import sys\n","sys.path.append('/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master')"],"metadata":{"id":"KMsxg3cvbcsj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["! pip3 install transformers"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Rz3lUwUCbnzx","executionInfo":{"status":"ok","timestamp":1680228916724,"user_tz":240,"elapsed":12859,"user":{"displayName":"kuma","userId":"09741713883164986443"}},"outputId":"f2932094-cc95-4f73-9178-69ac506d4c8d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m93.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m107.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"]}]},{"cell_type":"code","source":["from run_Bert_model import model_train_validate_test\n","import pandas as pd\n","from utils import Metric\n","import os\n","\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data\"\n","train_df = pd.read_csv(os.path.join(data_path,\"train.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","dev_df = pd.read_csv(os.path.join(data_path,\"dev.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","test_df = pd.read_csv(os.path.join(data_path,\"test.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","target_dir = \"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data/output/Bert/\"\n","\n","model_train_validate_test(train_df, dev_df, test_df, target_dir, \n","         max_seq_len=50,\n","         epochs=3,\n","         batch_size=32,\n","         lr=2e-05,\n","         patience=1,\n","         max_grad_norm=10.0,\n","         if_save_model=True,\n","         checkpoint=None)\n","\n","test_result = pd.read_csv(os.path.join(target_dir, 'test_prediction.csv'))\n","Metric(test_df.similarity, test_result.prediction) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N_BjCunPb4Qt","executionInfo":{"status":"ok","timestamp":1680229667251,"user_tz":240,"elapsed":213705,"user":{"displayName":"kuma","userId":"09741713883164986443"}},"outputId":"d206b614-a68f-4572-b88d-c92e39ad2d70"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================  Preparing for training  ====================\n","\t* Loading training data...\n","\t* Loading validation data...\n","\t* Loading test data...\n","\t* Building model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","* Validation loss before training: 0.3822, accuracy: 89.5642%, auc: 0.9547\n","\n"," ==================== Training bert model on device: cuda ====================\n","* Training epoch 1:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.2522s, loss: 0.1164: 100%|██████████| 217/217 [00:57<00:00,  3.75it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 57.8907s, loss = 0.1164, accuracy: 95.4624%\n","* Validation for epoch 1:\n","-> Valid. time: 2.2019s, loss: 0.2712, accuracy: 91.2844%, auc: 0.9684\n","\n","save model succesfully!\n","\n","* Test for epoch 1:\n","Test accuracy: 0.9319%\n","\n","* Training epoch 2:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.2628s, loss: 0.0416: 100%|██████████| 217/217 [01:00<00:00,  3.59it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 60.3828s, loss = 0.0416, accuracy: 98.6272%\n","* Validation for epoch 2:\n","-> Valid. time: 2.2555s, loss: 0.3219, accuracy: 91.7431%, auc: 0.9718\n","\n","save model succesfully!\n","\n","* Test for epoch 2:\n","Test accuracy: 0.9209%\n","\n","* Training epoch 3:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.2665s, loss: 0.0254: 100%|██████████| 217/217 [01:01<00:00,  3.55it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 61.1531s, loss = 0.0254, accuracy: 99.1908%\n","* Validation for epoch 3:\n","-> Valid. time: 2.3258s, loss: 0.4009, accuracy: 91.0550%, auc: 0.9714\n","\n","-> Early stopping: patience limit reached, stopping...\n","Accuracy: 92.1%\n","Precision: 92.2%\n","Recall: 92.1%\n","F1: 92.1%\n","classification_report:\n","\n","              precision    recall  f1-score   support\n","\n","     class_0      0.942     0.897     0.919       912\n","     class_1      0.901     0.945     0.923       909\n","\n","    accuracy                          0.921      1821\n","   macro avg      0.922     0.921     0.921      1821\n","weighted avg      0.922     0.921     0.921      1821\n","\n"]}]},{"cell_type":"markdown","source":["用cola数据集训练"],"metadata":{"id":"bbIpM8-tnd6w"}},{"cell_type":"code","source":["from run_Bert_model import model_train_validate_test\n","import pandas as pd\n","from utils import Metric\n","import os\n","\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data\"\n","train_df = pd.read_csv(os.path.join(data_path,\"in_domain_train_processed.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","dev_df = pd.read_csv(os.path.join(data_path,\"in_domain_dev_processed.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","test_df = pd.read_csv(os.path.join(data_path,\"out_of_domain_dev_processed.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","target_dir = \"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data/output/Bert/\"\n","\n","model_train_validate_test(train_df, dev_df, test_df, target_dir, \n","         max_seq_len=50,\n","         epochs=3,\n","         batch_size=32,\n","         lr=2e-05,\n","         patience=1,\n","         max_grad_norm=10.0,\n","         if_save_model=True,\n","         checkpoint=None)\n","\n","test_result = pd.read_csv(os.path.join(target_dir, 'test_prediction2.csv'))\n","Metric(test_df.similarity, test_result.prediction) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3yMZUBZbnhTI","executionInfo":{"status":"ok","timestamp":1680232098969,"user_tz":240,"elapsed":253724,"user":{"displayName":"kuma","userId":"09741713883164986443"}},"outputId":"3e68d50e-b20c-4554-d353-4e11214d0057"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["====================  Preparing for training  ====================\n","\t* Loading training data...\n","\t* Loading validation data...\n","\t* Loading test data...\n","\t* Building model...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","* Validation loss before training: 2.1761, accuracy: 42.6945%, auc: 0.5617\n","\n"," ==================== Training bert model on device: cuda ====================\n","* Training epoch 1:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.2491s, loss: 0.5518: 100%|██████████| 268/268 [01:10<00:00,  3.80it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 70.6021s, loss = 0.5518, accuracy: 74.1083%\n","* Validation for epoch 1:\n","-> Valid. time: 1.3456s, loss: 0.5046, accuracy: 76.8501%, auc: 0.8809\n","\n","save model succesfully!\n","\n","* Test for epoch 1:\n","Test accuracy: 0.7558%\n","\n","* Training epoch 2:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.2604s, loss: 0.3685: 100%|██████████| 268/268 [01:13<00:00,  3.63it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 73.7997s, loss = 0.3685, accuracy: 84.0720%\n","* Validation for epoch 2:\n","-> Valid. time: 1.4844s, loss: 0.4507, accuracy: 82.5427%, auc: 0.8902\n","\n","save model succesfully!\n","\n","* Test for epoch 2:\n","Test accuracy: 0.7926%\n","\n","* Training epoch 3:\n"]},{"output_type":"stream","name":"stderr","text":["Avg. batch proc. time: 0.2655s, loss: 0.2301: 100%|██████████| 268/268 [01:15<00:00,  3.56it/s]\n"]},{"output_type":"stream","name":"stdout","text":["-> Training time: 75.2114s, loss = 0.2301, accuracy: 90.7145%\n","* Validation for epoch 3:\n","-> Valid. time: 1.4519s, loss: 0.4832, accuracy: 83.3017%, auc: 0.8868\n","\n","save model succesfully!\n","\n","* Test for epoch 3:\n","Test accuracy: 0.7946%\n","\n","Accuracy: 79.5%\n","Precision: 79.8%\n","Recall: 70.3%\n","F1: 72.3%\n","classification_report:\n","\n","              precision    recall  f1-score   support\n","\n","     class_0      0.804     0.457     0.583       162\n","     class_1      0.792     0.949     0.864       354\n","\n","    accuracy                          0.795       516\n","   macro avg      0.798     0.703     0.723       516\n","weighted avg      0.796     0.795     0.776       516\n","\n"]}]},{"cell_type":"markdown","source":["原因：‘in 1:注意到，这里面的句子看起来不是很长，有些错误是性别不符，有些是缺词、少词，有些是加s不加s的情况，各种语法错误。但我也注意到，有一些看起来错误并没有那么严重，甚至在某些情况还是可以说的通的。in 2:注意到，由于句子来源于电影评论，又有它们情感的人类注释，不同于CoLA的整体偏短，有些句子很长，有些句子很短，长短并不整齐划一。’"],"metadata":{"id":"uurL6PS0pQN6"}},{"cell_type":"markdown","source":["MRPC数据集：本任务的数据集，包含两句话，每个样本的句子长度都非常长，且数据不均衡，正样本占比68%，负样本仅占32%。"],"metadata":{"id":"JaYo9wIlqtRa"}},{"cell_type":"code","source":["from run_Bert_model import model_train_validate_test\n","import pandas as pd\n","from utils import Metric\n","import os\n","\n","data_path = \"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data\"\n","train_df = pd.read_csv(os.path.join(data_path,\"msr_paraphrase_train.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","dev_df = pd.read_csv(os.path.join(data_path,\"msr_paraphrase_vali.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","test_df = pd.read_csv(os.path.join(data_path,\"msr_paraphrase_test.tsv\"),sep='\\t',header=None, names=['similarity','s1'])\n","target_dir = \"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data/output/Bert/\"\n","\n","model_train_validate_test(train_df, dev_df, test_df, target_dir, \n","         max_seq_len=50,\n","         epochs=3,\n","         batch_size=32,\n","         lr=2e-05,\n","         patience=1,\n","         max_grad_norm=10.0,\n","         if_save_model=True,\n","         checkpoint=None)\n","\n","test_result = pd.read_csv(os.path.join(target_dir, 'test_prediction3.csv'))\n","Metric(test_df.similarity, test_result.prediction) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":297},"id":"cBi5RvxJqx9i","executionInfo":{"status":"error","timestamp":1680234393939,"user_tz":240,"elapsed":775,"user":{"displayName":"kuma","userId":"09741713883164986443"}},"outputId":"b986ea71-0094-4fda-857d-deb7fcceb998"},"execution_count":null,"outputs":[{"output_type":"error","ename":"ParserError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mParserError\u001b[0m                               Traceback (most recent call last)","\u001b[0;32m<ipython-input-10-c0a781d0a8d9>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/drive/MyDrive/Colab Notebooks/final proj/SST-2-sentiment-analysis-master/data\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mtrain_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"msr_paraphrase_train.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m's1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mdev_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"msr_paraphrase_vali.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m's1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"msr_paraphrase_test.tsv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mheader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'similarity'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m's1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 678\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 581\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    582\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1251\u001b[0m             \u001b[0mnrows\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalidate_integer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"nrows\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1252\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1253\u001b[0;31m                 \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcol_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1254\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1255\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlow_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 225\u001b[0;31m                 \u001b[0mchunks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_low_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    226\u001b[0m                 \u001b[0;31m# destructive to chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_concatenate_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[0;34m()\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[0;34m()\u001b[0m\n","\u001b[0;31mParserError\u001b[0m: Error tokenizing data. C error: Expected 2 fields in line 100, saw 3\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"7OQKruX5qs-U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ***Limitations***"],"metadata":{"id":"SGpJ2O73lbTK"}},{"cell_type":"markdown","source":["Some limitations of the papers include:\n","\n","The models require a large amount of training data and computational resources.\n","They are prone to generating plausible-sounding but incorrect or nonsensical text.\n","They have difficulty in handling out-of-vocabulary words and rare or unseen words.\n","There is limited interpretability and explainability of the self-attention mechanism.\n","\n","局限性：\n","基于Transformer的模型的局限性在于，它们需要大量的预训练数据和计算资源，这使得较小的研究小组或个人不容易获得。另一个限制是，这些模型可能不能很好地推广到新的任务或领域，需要进行大量的微调工作。\n","\n"],"metadata":{"id":"SJV4_HxvldVO"}},{"cell_type":"markdown","source":["# ***Future directions***"],"metadata":{"id":"cOdGgJpBle5X"}},{"cell_type":"markdown","source":["Based on the limitations, the following directions can be considered for future research:\n","\n","Investigate techniques for improving the efficiency and reducing the computational requirements of the models, allowing them to be trained on less data and with fewer resources.\n","Explore methods for increasing the robustness of the models to noisy or out-of-domain input text.\n","Develop approaches to improve the interpretability and explainability of the self-attention mechanism.\n","Investigate methods for fine-tuning and adapting the models to specific tasks or domains with limited data.\n","Explore ways to incorporate additional sources of information, such as knowledge graphs, to improve the model's understanding of context and semantics.\n","\n","基于这些局限性，在未来的研究中可以考虑以下方向：\n","\n","研究提高模型效率和降低计算要求的技术，使其能够在更少的数据和更少的资源上进行训练。\n","探索提高模型对噪声或域外输入文本的稳健性的方法。\n","制定方法来提高自我注意机制的可解释性和可说明性。\n","研究如何在数据有限的情况下对模型进行微调并使之适应特定的任务或领域。\n","探讨如何纳入额外的信息来源，如知识图谱，以提高模型对背景和语义的理解。"],"metadata":{"id":"Q9q1TC2dlgkl"}},{"cell_type":"code","source":[],"metadata":{"id":"6_87W6TPlBfR"},"execution_count":null,"outputs":[]}]}